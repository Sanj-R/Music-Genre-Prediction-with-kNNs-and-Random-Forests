{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "14p8_xstbodFrMRdmwXObu6zexuxo1UFf",
      "authorship_tag": "ABX9TyMZUopYwFUEhdK7PKlOc+4J",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sanj-R/Music-Genre-Prediction-with-kNNs-and-Random-Forests/blob/main/all_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CODE FOR DATA EXPLORATION SECTION**"
      ],
      "metadata": {
        "id": "AwCZzAfc4TPB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qm7bMWGJvfzG"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_train_names_labels = pd.read_csv('train.csv')"
      ],
      "metadata": {
        "id": "Tau3nPhnyXJZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "genres = ['blues', 'classical', 'country', 'disco', 'hiphop', 'jazz', 'metal', 'pop', 'reggae', 'rock']"
      ],
      "metadata": {
        "id": "qXoFixJIyvHT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "duplicates = np.array([[6, 179],\n",
        "[20, 785],\n",
        "[49, 431],\n",
        "[50, 288],\n",
        "[60, 437],\n",
        "[82, 390],\n",
        "[113, 261],\n",
        "[116, 165],\n",
        "[123, 395],\n",
        "[270, 312],\n",
        "[293, 737],\n",
        "[343, 584]])"
      ],
      "metadata": {
        "id": "u0vHAGXx8Bqe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the ground truth y vector containing the genres of all training data as numbers (e.g. 'blues' is 0, 'classical' is 1, etc.)\n",
        "\n",
        "all_train_numer_labels = []\n",
        "for i in range(800):\n",
        "  if i in duplicates[:,1]:\n",
        "    continue\n",
        "  genre = all_train_names_labels['Genre'].iloc[i]\n",
        "  numer_genre = genres.index(genre)\n",
        "  all_train_numer_labels.append(np.array([numer_genre]))\n",
        "all_train_numer_labels = np.concatenate(all_train_numer_labels, axis=0)\n",
        "print(all_train_numer_labels.shape)\n",
        "print(all_train_numer_labels.dtype)\n",
        "print(all_train_numer_labels)"
      ],
      "metadata": {
        "id": "G-eaUjjE0J_U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the number of training data songs of each genre\n",
        "\n",
        "nums_labels = {}\n",
        "for i in range(all_train_numer_labels.size):\n",
        "  label = all_train_numer_labels[i]\n",
        "  nums_labels[label] = nums_labels.get(label, 0) + 1\n",
        "print(nums_labels)"
      ],
      "metadata": {
        "id": "ARkDlaIa0Kc2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy"
      ],
      "metadata": {
        "id": "s3yb0XIn0NDt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify the sample rates, sample lengths, and the given data types of the training data points\n",
        "\n",
        "sample_rates = {}\n",
        "lengths = {}\n",
        "types = {}\n",
        "\n",
        "for i in range(800):\n",
        "  if i in duplicates[:,1]:\n",
        "    continue\n",
        "  data = scipy.io.wavfile.read(f'train{i:03}.wav')\n",
        "  sample_rates[data[0]] = sample_rates.get(data[0], 0) + 1\n",
        "  lengths[data[1].shape] = lengths.get(data[1].shape, 0) + 1\n",
        "  types[data[1].dtype] = types.get(data[1].dtype, 0) + 1\n",
        "\n",
        "print(sample_rates)\n",
        "print(lengths)\n",
        "print(types)\n",
        "print()\n",
        "print(sorted(list(lengths)))"
      ],
      "metadata": {
        "id": "1j3M_gPk0NdU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#min and max durations, and we now know 9 of the samples have < 30 secs duration\n",
        "\n",
        "print(660000/22050)\n",
        "print(675808/22050)\n",
        "print(22050*30)"
      ],
      "metadata": {
        "id": "ED9pCIwd0PGg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# trim or extend all the training data to be 30 seconds long\n",
        "\n",
        "import scipy\n",
        "\n",
        "all_train_raw_data_matrix = [] #it will also be trimmed/extended to 30 secs for all clips\n",
        "lengths = set()\n",
        "\n",
        "for i in range(800):\n",
        "  if i in duplicates[:,1]:\n",
        "    continue\n",
        "  data = scipy.io.wavfile.read(f'train{i:03}.wav')[1]\n",
        "  if data.size < 661500:\n",
        "    data = np.pad(data, (0,661500-data.size))\n",
        "  elif data.size > 661500:\n",
        "    data = data[:661500]\n",
        "  data = data.reshape((1,-1))\n",
        "  lengths.add(data.shape)\n",
        "  all_train_raw_data_matrix.append(data)\n",
        "\n",
        "all_train_raw_data_matrix = np.concatenate(all_train_raw_data_matrix, axis=0)\n",
        "print(lengths)\n",
        "print(all_train_raw_data_matrix.shape)"
      ],
      "metadata": {
        "id": "PzRTafX20TCh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CODE FOR FEATURE EXTRACTION SECTION**"
      ],
      "metadata": {
        "id": "g_xmS5WN4Z_Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "CODE FOR RAW DATA SUBSECTION"
      ],
      "metadata": {
        "id": "QE-VcgKX6gNn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#we stratified split 75/25, so overall, 60% train, 20% val, 10% public test, 10% private test\n",
        "\n",
        "import sklearn.model_selection\n",
        "\n",
        "train_raw_data_matrix, val_raw_data_matrix, train_labels, val_labels = sklearn.model_selection.train_test_split(all_train_raw_data_matrix, all_train_numer_labels, train_size=0.75, random_state=123456, stratify=all_train_numer_labels)\n",
        "\n",
        "print(train_raw_data_matrix.shape)\n",
        "print(val_raw_data_matrix.shape)\n",
        "print(train_labels.shape)\n",
        "print(val_labels.shape)\n",
        "print()\n",
        "\n",
        "nums_labels = {}\n",
        "for i in range(train_labels.size):\n",
        "  label = train_labels[i]\n",
        "  nums_labels[label] = nums_labels.get(label, 0) + 1\n",
        "print(nums_labels)\n",
        "\n",
        "nums_labels = {}\n",
        "for i in range(val_labels.size):\n",
        "  label = val_labels[i]\n",
        "  nums_labels[label] = nums_labels.get(label, 0) + 1\n",
        "print(nums_labels)"
      ],
      "metadata": {
        "id": "tvjxKgj54pTA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Store color array for plotting later\n",
        "\n",
        "import sklearn.decomposition\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "colors = np.array(['red', 'blue', 'green', 'yellow', 'black', 'gray', 'magenta', 'pink', 'orange', 'brown'])"
      ],
      "metadata": {
        "id": "EcOxI5dh6qU8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert raw data into 2-component PCA projection\n",
        "\n",
        "pca = sklearn.decomposition.PCA(n_components=2)\n",
        "pca_train_raw_data_matrix = pca.fit_transform(train_raw_data_matrix)"
      ],
      "metadata": {
        "id": "sDhpWMlv7O1y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot PCA (2 components) Projection of Centered Raw Data\n",
        "\n",
        "plt.scatter(pca_train_raw_data_matrix[:,0], pca_train_raw_data_matrix[:,1], c=colors[train_labels])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Q8apSvc97RxU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Try to identify the indices of the outlying points\n",
        "\n",
        "for i in range(train_labels.size):\n",
        "  if pca_train_raw_data_matrix[i,1] > 8*1e6 or pca_train_raw_data_matrix[i,0] > 1*1e7:\n",
        "    print(i, np.multiply(pca_train_raw_data_matrix[i], np.array([1/1e7, 1/1e6])))"
      ],
      "metadata": {
        "id": "J1pRMd9872tS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the scatterplot after Removing Outliers from PCA (2 components) Projection of Centered Raw Data\n",
        "\n",
        "trim_boolean = np.array(train_labels.size*[True])\n",
        "trim_boolean[205] = False\n",
        "trim_boolean[242] = False\n",
        "\n",
        "plt.scatter(pca_train_raw_data_matrix[trim_boolean][:,0], pca_train_raw_data_matrix[trim_boolean][:,1], c=colors[train_labels[trim_boolean]])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "O51_ClCm8Qnd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the PCA (2 components) Projection of Centered Non-Outlier Raw Data\n",
        "\n",
        "pca = sklearn.decomposition.PCA(n_components=2)\n",
        "pca_trim_train_raw_data_matrix = pca.fit_transform(train_raw_data_matrix[trim_boolean])\n",
        "\n",
        "plt.scatter(pca_trim_train_raw_data_matrix[:,0], pca_trim_train_raw_data_matrix[:,1], c=colors[train_labels[trim_boolean]])"
      ],
      "metadata": {
        "id": "b6Dl2t5L8pB9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Try to identify the indices of the outlying points from this plot\n",
        "\n",
        "for i in range(train_labels[trim_boolean].size):\n",
        "  if pca_trim_train_raw_data_matrix[i,1] > 6*1e6 or pca_trim_train_raw_data_matrix[i,0] > 6*1e6:\n",
        "    print(i, np.multiply(pca_trim_train_raw_data_matrix[i], np.array([1/1e6, 1/1e6])))"
      ],
      "metadata": {
        "id": "P2eTfJQS84CI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the scatterplot after Removing Outliers from PCA (2 components) Projection of Centered Non-Outlier Raw Data\n",
        "\n",
        "trim_boolean2 = np.array(train_labels[trim_boolean].size*[True])\n",
        "trim_boolean2[186] = False\n",
        "trim_boolean2[489] = False\n",
        "\n",
        "plt.scatter(pca_trim_train_raw_data_matrix[trim_boolean2][:,0], pca_trim_train_raw_data_matrix[trim_boolean2][:,1], c=colors[train_labels[trim_boolean][trim_boolean2]])"
      ],
      "metadata": {
        "id": "S3RauTEA9E8M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CODE FOR 3-SECOND AUDIO CLIPS SUBSECTION"
      ],
      "metadata": {
        "id": "uqk7dJeJ9ttv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Divide raw training data into 3-second data points\n",
        "\n",
        "ts_all_train_raw_data_matrix = []\n",
        "for i in range(0, all_train_raw_data_matrix.shape[1], int(all_train_raw_data_matrix.shape[1]/10)):\n",
        "  mat_slice = all_train_raw_data_matrix[:,i:i+int(all_train_raw_data_matrix.shape[1]/10)]\n",
        "  ts_all_train_raw_data_matrix.append(mat_slice)\n",
        "ts_all_train_raw_data_matrix = np.concatenate(ts_all_train_raw_data_matrix, axis=0)\n",
        "ts_all_train_numer_labels = np.concatenate(10*[all_train_numer_labels], axis=0).copy()\n",
        "\n",
        "print(ts_all_train_raw_data_matrix.shape)\n",
        "print(ts_all_train_numer_labels.shape)"
      ],
      "metadata": {
        "id": "OklkOP5e-Fz8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#we stratified split 75/25 (ignoring the 10x splits), so overall, 60% train, 20% val, 10% public test, 10% private test\n",
        "\n",
        "import sklearn.model_selection\n",
        "\n",
        "ts_train_raw_data_matrix, ts_val_raw_data_matrix, ts_train_labels, ts_val_labels = sklearn.model_selection.train_test_split(ts_all_train_raw_data_matrix, ts_all_train_numer_labels, train_size=0.75, random_state=123456, stratify=ts_all_train_numer_labels)\n",
        "\n",
        "print(ts_train_raw_data_matrix.shape)\n",
        "print(ts_val_raw_data_matrix.shape)\n",
        "print(ts_train_labels.shape)\n",
        "print(ts_val_labels.shape)\n",
        "print()\n",
        "\n",
        "nums_labels = {}\n",
        "for i in range(ts_train_labels.size):\n",
        "  label = ts_train_labels[i]\n",
        "  nums_labels[label] = nums_labels.get(label, 0) + 1\n",
        "print(nums_labels)\n",
        "\n",
        "nums_labels = {}\n",
        "for i in range(ts_val_labels.size):\n",
        "  label = ts_val_labels[i]\n",
        "  nums_labels[label] = nums_labels.get(label, 0) + 1\n",
        "print(nums_labels)"
      ],
      "metadata": {
        "id": "vpvwvkV3-NpF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Store 3-second data for future use (in reality we didn't need to save this data)\n",
        "\n",
        "mdic = {'ts_train_raw_data_matrix':ts_train_raw_data_matrix, 'ts_val_raw_data_matrix':ts_val_raw_data_matrix, 'ts_train_labels':ts_train_labels, 'ts_val_labels':ts_val_labels}\n",
        "\n",
        "import scipy.io\n",
        "\n",
        "scipy.io.savemat('3sec_train_val_data.mat', mdic)"
      ],
      "metadata": {
        "id": "FEiOTm9E-fhD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the PCA (2 components) Projection of Centered Raw 3-Second Clips\n",
        "\n",
        "import sklearn.decomposition\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "colors = np.array(['red', 'blue', 'green', 'yellow', 'black', 'gray', 'magenta', 'pink', 'orange', 'brown'])\n",
        "pca = sklearn.decomposition.PCA(n_components=2)\n",
        "pca_train_raw_data_matrix = pca.fit_transform(ts_train_raw_data_matrix)\n",
        "plt.scatter(pca_train_raw_data_matrix[:,0], pca_train_raw_data_matrix[:,1], c=colors[ts_train_labels])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-GKkWaK6BTZ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CODE FOR MFCCs of 5-Second Audio Clips SUBSECTION"
      ],
      "metadata": {
        "id": "yaaPhkVYBsOT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the ground truth y vector containing the genres of all training data as numbers (e.g. 'blues' is 0, 'classical' is 1, etc.)\n",
        "# This one contains duplicates too\n",
        "\n",
        "all_train_numer_labels = []\n",
        "for i in range(800):\n",
        "  genre = all_train_names_labels['Genre'].iloc[i]\n",
        "  numer_genre = genres.index(genre)\n",
        "  all_train_numer_labels.append(np.array([numer_genre]))\n",
        "all_train_numer_labels = np.concatenate(all_train_numer_labels, axis=0)\n",
        "print(all_train_numer_labels.shape)\n",
        "print(all_train_numer_labels.dtype)\n",
        "print(all_train_numer_labels)"
      ],
      "metadata": {
        "id": "gIqLX8DVBupl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# trim or extend all the training data to be 30 seconds long\n",
        "\n",
        "import scipy\n",
        "\n",
        "all_train_raw_data_matrix = [] #it will also be trimmed/extended to 30 secs for all clips\n",
        "lengths = set()\n",
        "\n",
        "for i in range(800):\n",
        "  data = scipy.io.wavfile.read(f'train{i:03}.wav')[1]\n",
        "  if data.size < 661500:\n",
        "    data = np.pad(data, (0,661500-data.size))\n",
        "  elif data.size > 661500:\n",
        "    data = data[:661500]\n",
        "  data = data.reshape((1,-1))\n",
        "  lengths.add(data.shape)\n",
        "  all_train_raw_data_matrix.append(data)\n",
        "\n",
        "all_train_raw_data_matrix = np.concatenate(all_train_raw_data_matrix, axis=0)\n",
        "print(lengths)\n",
        "print(all_train_raw_data_matrix.shape)"
      ],
      "metadata": {
        "id": "Vm2Y2YjoCWhF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Allows splitting 30 second songs into (30/shrink_factor) second clips\n",
        "\n",
        "def split_to_smaller_data(X, y, shrink_factor):\n",
        "  new_X = []\n",
        "  for i in range(0, X.shape[1], int(X.shape[1]/shrink_factor)):\n",
        "    mat_slice = X[:,i:i+int(X.shape[1]/shrink_factor)]\n",
        "    new_X.append(mat_slice)\n",
        "  new_X = np.concatenate(new_X, axis=0)\n",
        "  new_y = np.concatenate(shrink_factor*[y], axis=0).copy()\n",
        "  return new_X, new_y"
      ],
      "metadata": {
        "id": "zYL3U0CbChZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check shapes if data was split to 5 second clips, to make sure it's working correctly\n",
        "\n",
        "print(split_to_smaller_data(all_train_raw_data_matrix, all_train_numer_labels, 6)[0].shape)\n",
        "print(split_to_smaller_data(all_train_raw_data_matrix, all_train_numer_labels, 6)[1].shape)"
      ],
      "metadata": {
        "id": "e3pB_UA6CwVm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save 5-second training data (this turned out to be unnecessary)\n",
        "\n",
        "mdic = {'X':split_to_smaller_data(all_train_raw_data_matrix, all_train_numer_labels, 6)[0],\n",
        "        'y':split_to_smaller_data(all_train_raw_data_matrix, all_train_numer_labels, 6)[1]}\n",
        "\n",
        "import scipy.io\n",
        "\n",
        "scipy.io.savemat('5sec_train.mat', mdic)"
      ],
      "metadata": {
        "id": "wVuGX1XYC4x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load saved 5-second training data\n",
        "\n",
        "import scipy.io\n",
        "\n",
        "f_sec_data = scipy.io.loadmat('5sec_train.mat')\n",
        "\n",
        "all_X = f_sec_data['X'].astype(float)\n",
        "\n",
        "print(all_X.shape)\n",
        "del f_sec_data"
      ],
      "metadata": {
        "id": "ZdVFLUIwDJCe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create MFCC features (20 for each 5-second datapoint)\n",
        "\n",
        "import librosa.feature\n",
        "import numpy as np\n",
        "\n",
        "full_mfcc_X = []\n",
        "cnt = 0\n",
        "for data_point in all_X:\n",
        "  a = np.mean(librosa.feature.mfcc(y=data_point), axis=1).reshape((1,-1))\n",
        "  full_mfcc_X.append(a)\n",
        "  cnt+=1\n",
        "  if cnt % 100 == 0:\n",
        "    print(cnt)"
      ],
      "metadata": {
        "id": "98NtDAWzDx1m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert MFCC data into ndarray, and check shape of ndarray\n",
        "\n",
        "full_mfcc_X = np.concatenate(full_mfcc_X, axis=0)\n",
        "\n",
        "print()\n",
        "print(full_mfcc_X.shape)"
      ],
      "metadata": {
        "id": "KyGwoNr4D-LW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save MFCC data\n",
        "\n",
        "mdic = {'X':full_mfcc_X,\n",
        "        'y':scipy.io.loadmat('5sec_train.mat')['y']}\n",
        "\n",
        "import scipy.io\n",
        "\n",
        "scipy.io.savemat('mfcc_5sec_train.mat', mdic)"
      ],
      "metadata": {
        "id": "kzuPBtSsEAjm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Center, Normalize, and shuffle MFCC data\n",
        "\n",
        "import scipy.io\n",
        "import sklearn.preprocessing\n",
        "import sklearn.utils\n",
        "\n",
        "mfcc_f_sec_data = scipy.io.loadmat('mfcc_5sec_train.mat')\n",
        "\n",
        "all_mfcc_X = mfcc_f_sec_data['X'].astype(float)\n",
        "all_y = mfcc_f_sec_data['y'].reshape(-1,)\n",
        "\n",
        "scaler = sklearn.preprocessing.StandardScaler()\n",
        "all_mfcc_X = scaler.fit_transform(all_mfcc_X)\n",
        "\n",
        "all_mfcc_X, all_y = sklearn.utils.shuffle(all_mfcc_X, all_y, random_state=123456)"
      ],
      "metadata": {
        "id": "yRkfPinlEMx5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot PCA (2 components) Projection of Centered MFCCs of 5-Second Clips\n",
        "\n",
        "import sklearn.decomposition\n",
        "import numpy as np\n",
        "\n",
        "pca_2 = sklearn.decomposition.PCA(n_components=2)\n",
        "pca_2_all_mfcc_X = pca_2.fit_transform(all_mfcc_X)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "colors = np.array(['red', 'blue', 'green', 'yellow', 'black', 'gray', 'magenta', 'pink', 'orange', 'brown'])\n",
        "\n",
        "plt.scatter(pca_2_all_mfcc_X[:,0], pca_2_all_mfcc_X[:,1], c=colors[all_y])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Zk_qEh5YEe8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CODE FOR K-Nearest Neighbors SUBSECTION**\n"
      ],
      "metadata": {
        "id": "g7f6fxWfHpY4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute KNN 5-fold cross validation scores from different hyper-parameter combinations\n",
        "# Uses MFCC 5-second data\n",
        "\n",
        "import sklearn.model_selection\n",
        "import sklearn.neighbors\n",
        "\n",
        "for n_components in (2,3,5,7,9,11,14,17,20):\n",
        "  pca = sklearn.decomposition.PCA(n_components=n_components)\n",
        "  pca_all_mfcc_X = pca.fit_transform(all_mfcc_X)\n",
        "  for n_neighbors in [1,2,5,10,25,50,100,250,500,1000,2500,3800]:\n",
        "    knn = sklearn.neighbors.KNeighborsClassifier(n_neighbors=n_neighbors)\n",
        "    knn_cv = sklearn.model_selection.cross_validate(knn, pca_all_mfcc_X, all_y)\n",
        "    print(n_components, n_neighbors, knn_cv['test_score'], np.mean(knn_cv['test_score']))\n",
        "  print()"
      ],
      "metadata": {
        "id": "C-1B6O25Gbrb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load MFCC 5-second training data\n",
        "\n",
        "import scipy.io\n",
        "import sklearn.preprocessing\n",
        "import sklearn.utils\n",
        "\n",
        "mfcc_f_sec_data = scipy.io.loadmat('mfcc_5sec_train.mat')\n",
        "\n",
        "all_mfcc_X = mfcc_f_sec_data['X'].astype(float)\n",
        "all_y = mfcc_f_sec_data['y'].reshape(-1,)\n",
        "\n",
        "scaler = sklearn.preprocessing.StandardScaler()\n",
        "all_mfcc_X = scaler.fit_transform(all_mfcc_X)\n",
        "\n",
        "all_mfcc_X, all_y = sklearn.utils.shuffle(all_mfcc_X, all_y, random_state=123456)"
      ],
      "metadata": {
        "id": "Nx3uFeB2OSk6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Output and reshape the 5-second MFCC testing data predictions\n",
        "\n",
        "import librosa.feature\n",
        "import numpy as np\n",
        "\n",
        "test_data_matrix = []\n",
        "\n",
        "for i in range(200):\n",
        "  data = scipy.io.wavfile.read(f'test{i:03}.wav')[1]\n",
        "  if data.size < 661500:\n",
        "    data = np.pad(data, (0,661500-data.size))\n",
        "  elif data.size > 661500:\n",
        "    data = data[:661500]\n",
        "  data = data.reshape((1,-1))\n",
        "  test_data_matrix.append(data)\n",
        "\n",
        "test_data_matrix = np.concatenate(test_data_matrix, axis=0)\n",
        "print(test_data_matrix.shape)\n",
        "\n",
        "\n",
        "\n",
        "test_data_matrix = split_to_smaller_data(test_data_matrix, np.zeros((test_data_matrix.shape[0],)), 6)[0]\n",
        "\n",
        "\n",
        "\n",
        "test_data_matrix = test_data_matrix.astype(float)\n",
        "\n",
        "test_full_mfcc_X = []\n",
        "cnt = 0\n",
        "for data_point in test_data_matrix:\n",
        "  a = np.mean(librosa.feature.mfcc(y=data_point), axis=1).reshape((1,-1))\n",
        "  test_full_mfcc_X.append(a)\n",
        "  cnt+=1\n",
        "  if cnt % 100 == 0:\n",
        "    print(cnt)\n",
        "\n",
        "test_full_mfcc_X = np.concatenate(test_full_mfcc_X, axis=0)\n",
        "\n",
        "print()\n",
        "print(test_full_mfcc_X.shape)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "small_test_predictions = rf.predict(scaler.transform(test_full_mfcc_X)).reshape((-1,))\n",
        "print(small_test_predictions.reshape((6,-1)))"
      ],
      "metadata": {
        "id": "_2S26pOXOChf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the KNN with the optimmal hyperparameter settings\n",
        "\n",
        "import sklearn.neighbors\n",
        "\n",
        "knn = sklearn.neighbors.KNeighborsClassifier(n_neighbors=1)\n",
        "knn.fit(all_mfcc_X, all_y)\n",
        "knn.score(all_mfcc_X, all_y)"
      ],
      "metadata": {
        "id": "A5e9ZD1rOW2h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict the 5-second test data with the KNN\n",
        "\n",
        "small_test_predictions = knn.predict(scaler.transform(test_full_mfcc_X)).reshape((-1,))\n",
        "print(small_test_predictions.reshape((6,-1)))"
      ],
      "metadata": {
        "id": "NYuKXKKlOmIm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Format test data predictions appropriately\n",
        "\n",
        "import scipy.stats\n",
        "import pandas as pd\n",
        "\n",
        "test_pred = scipy.stats.mode(small_test_predictions.reshape((6,-1)), axis=0).mode\n",
        "\n",
        "test_predictions_1 = pd.DataFrame(np.concatenate([np.array([f'test{i:03}.wav' for i in range(200)]).reshape((-1,1)), (np.array(genres)[test_pred]).reshape((-1,1))], axis=1), columns=['ID', 'Genre'])"
      ],
      "metadata": {
        "id": "52EXbuAiOvYD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Output predictions as csv\n",
        "\n",
        "test_predictions_1.to_csv('knn_mfcc_test_predictions_1.csv', index=False)"
      ],
      "metadata": {
        "id": "UXvDpQmiOyaD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CODE FOR RANDOM FOREST SUBSECTION**"
      ],
      "metadata": {
        "id": "ypnRvGn4I3E2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Try Random Forest on MFCC 5-second data with default hyperparameters, to check if random forest is a viable method\n",
        "\n",
        "import sklearn.ensemble\n",
        "\n",
        "rf_cv = sklearn.model_selection.cross_validate(sklearn.ensemble.RandomForestClassifier(), pca_all_mfcc_X, all_y)\n",
        "print(rf_cv['test_score'], np.mean(rf_cv['test_score']))"
      ],
      "metadata": {
        "id": "DtqivktxIpSY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Do Random Forest with different hyperparameter combinations\n",
        "\n",
        "hyperparam_choices = {\n",
        "'n_estimators' : [25, 50, 100, 250],\n",
        "'max_depth' : [3, 5, 10, 15, None],\n",
        "}\n",
        "\n",
        "rf_grid_search = sklearn.model_selection.GridSearchCV(sklearn.ensemble.RandomForestClassifier(), param_grid=hyperparam_choices)\n",
        "rf_grid_search.fit(pca_all_mfcc_X, all_y)\n",
        "print(rf_grid_search.best_estimator_)\n",
        "print(rf_grid_search.cv_results_['mean_test_score'])"
      ],
      "metadata": {
        "id": "FE2fpwBqJGPg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Do Random Forest with 10k decision trees (WARNING, THIS TOOK ME 20-30 MIN TO RUN)\n",
        "\n",
        "hyperparam_choices = {\n",
        "'n_estimators' : [10000],\n",
        "}\n",
        "\n",
        "rf_grid_search = sklearn.model_selection.GridSearchCV(sklearn.ensemble.RandomForestClassifier(), param_grid=hyperparam_choices)\n",
        "rf_grid_search.fit(pca_all_mfcc_X, all_y)\n",
        "print(rf_grid_search.best_estimator_)\n",
        "print(rf_grid_search.cv_results_['mean_test_score'])"
      ],
      "metadata": {
        "id": "46PPMpClJRQK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load MFCC 5-second training data\n",
        "\n",
        "import scipy.io\n",
        "import sklearn.preprocessing\n",
        "import sklearn.utils\n",
        "\n",
        "mfcc_f_sec_data = scipy.io.loadmat('mfcc_5sec_train.mat')\n",
        "\n",
        "all_mfcc_X = mfcc_f_sec_data['X'].astype(float)\n",
        "all_y = mfcc_f_sec_data['y'].reshape(-1,)\n",
        "\n",
        "scaler = sklearn.preprocessing.StandardScaler()\n",
        "all_mfcc_X = scaler.fit_transform(all_mfcc_X)\n",
        "\n",
        "all_mfcc_X, all_y = sklearn.utils.shuffle(all_mfcc_X, all_y, random_state=123456)"
      ],
      "metadata": {
        "id": "eA4QQSnbLKgl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the Random Forest model with the settings which we found to work before\n",
        "\n",
        "import sklearn.ensemble\n",
        "\n",
        "rf = sklearn.ensemble.RandomForestClassifier(n_estimators=10000)\n",
        "rf.fit(all_mfcc_X, all_y)\n",
        "rf.score(all_mfcc_X, all_y)"
      ],
      "metadata": {
        "id": "BGNX6ex4NQL8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Output and reshape the 5-second MFCC testing data predictions\n",
        "\n",
        "import librosa.feature\n",
        "import numpy as np\n",
        "\n",
        "test_data_matrix = []\n",
        "\n",
        "for i in range(200):\n",
        "  data = scipy.io.wavfile.read(f'test{i:03}.wav')[1]\n",
        "  if data.size < 661500:\n",
        "    data = np.pad(data, (0,661500-data.size))\n",
        "  elif data.size > 661500:\n",
        "    data = data[:661500]\n",
        "  data = data.reshape((1,-1))\n",
        "  test_data_matrix.append(data)\n",
        "\n",
        "test_data_matrix = np.concatenate(test_data_matrix, axis=0)\n",
        "print(test_data_matrix.shape)\n",
        "\n",
        "\n",
        "\n",
        "test_data_matrix = split_to_smaller_data(test_data_matrix, np.zeros((test_data_matrix.shape[0],)), 6)[0]\n",
        "\n",
        "\n",
        "\n",
        "test_data_matrix = test_data_matrix.astype(float)\n",
        "\n",
        "test_full_mfcc_X = []\n",
        "cnt = 0\n",
        "for data_point in test_data_matrix:\n",
        "  a = np.mean(librosa.feature.mfcc(y=data_point), axis=1).reshape((1,-1))\n",
        "  test_full_mfcc_X.append(a)\n",
        "  cnt+=1\n",
        "  if cnt % 100 == 0:\n",
        "    print(cnt)\n",
        "\n",
        "test_full_mfcc_X = np.concatenate(test_full_mfcc_X, axis=0)\n",
        "\n",
        "print()\n",
        "print(test_full_mfcc_X.shape)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "small_test_predictions = rf.predict(scaler.transform(test_full_mfcc_X)).reshape((-1,))\n",
        "print(small_test_predictions.reshape((6,-1)))"
      ],
      "metadata": {
        "id": "hKPL9NMaNZHa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert predictions to appropriate format\n",
        "\n",
        "import scipy.stats\n",
        "import pandas as pd\n",
        "\n",
        "test_pred = scipy.stats.mode(small_test_predictions.reshape((6,-1)), axis=0).mode\n",
        "\n",
        "test_predictions_1 = pd.DataFrame(np.concatenate([np.array([f'test{i:03}.wav' for i in range(200)]).reshape((-1,1)), (np.array(genres)[test_pred]).reshape((-1,1))], axis=1), columns=['ID', 'Genre'])"
      ],
      "metadata": {
        "id": "O-Z2FqEjNp5S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Output predictions to csv\n",
        "\n",
        "test_predictions_1.to_csv('rf_test_predictions_2.csv', index=False)"
      ],
      "metadata": {
        "id": "W_lpJ2J3N0z7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}